{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ce62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from resnet import ResNet18, ResidualBlock\n",
    "from mobilenet import MobileNetV1, MobileNetV2\n",
    "from vggnet import VGG16\n",
    "from data import ImgDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a81344bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reproducibility settings\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353b38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"/home/banana9205/Desktop/Main/Uni/DATH/Dataset/plantvillage dataset/dataframes\"\n",
    "IMG_PATH = \"/home/banana9205/Desktop/Main/Uni/DATH/Dataset/plantvillage dataset\"\n",
    "TRAIN_IMG = os.path.join(IMG_PATH, \"train\")\n",
    "VAL_IMG = os.path.join(IMG_PATH, \"val\")\n",
    "TEST_IMG = os.path.join(IMG_PATH, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c70ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to training CSV: /home/banana9205/Desktop/Main/Uni/DATH/Dataset/plantvillage dataset/dataframes/train_labels.csv\n",
      "Path to validation CSV: /home/banana9205/Desktop/Main/Uni/DATH/Dataset/plantvillage dataset/dataframes/val_labels.csv\n",
      "Path to test CSV: /home/banana9205/Desktop/Main/Uni/DATH/Dataset/plantvillage dataset/dataframes/test_labels.csv\n"
     ]
    }
   ],
   "source": [
    "train_csv_path = os.path.join(CSV_PATH, \"train_labels.csv\")\n",
    "val_csv_path = os.path.join(CSV_PATH, \"val_labels.csv\")\n",
    "test_csv_path = os.path.join(CSV_PATH, \"test_labels.csv\")\n",
    "\n",
    "print(f\"Path to training CSV: {train_csv_path}\")\n",
    "print(f\"Path to validation CSV: {val_csv_path}\")\n",
    "print(f\"Path to test CSV: {test_csv_path}\")\n",
    "\n",
    "train_csv = pandas.read_csv(train_csv_path)\n",
    "val_csv = pandas.read_csv(val_csv_path)\n",
    "test_csv = pandas.read_csv(test_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02c5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization stats for ImageNet pretrained models\n",
    "# mean and std for each channel (R, G, B)\n",
    "#\n",
    "# If we want optimal values, we would have to compute mean and std over our dataset,\n",
    "# but it would take a long time. Thus we use ImageNet stats as an approximation.\n",
    "#\n",
    "# Some notes on why normalization is important:\n",
    "#   - Normalization, in this case, brings the pixel values to approximately [-1, 1] range.\n",
    "#\n",
    "#   - If left unnormalized, pixel values would be in [0, 255] range after ToTensor(), which\n",
    "#     can lead to skewed activations in the network and large variance/unstable gradients.\n",
    "#\n",
    "#   - A [-1, 1] range (hopefully) makes things easier for the model, as it centers the data\n",
    "#     around 0 and is symmetric, which helps with convergence during training, as gradients \n",
    "#     are more balanced.\n",
    "\n",
    "stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "# Here we provide 3 modes of data augmentation for training dataset\n",
    "# Mild augmentation: horizontal and vertical flips\n",
    "# Moderate augmentation: flips + random rotations + crops + color jitter\n",
    "# Aggressive augmentation: flips + random rotations + crops + color jitter + gaussian blur\n",
    "\n",
    "mild_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])\n",
    "\n",
    "moderate_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "\n",
    "    T.RandomRotation(90),\n",
    "    T.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.02),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])\n",
    "\n",
    "aggressive_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "\n",
    "    T.RandomRotation(180),\n",
    "    T.RandomResizedCrop(256, scale=(0.3, 1.0)),\n",
    "    T.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.03),\n",
    "\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0))], p=0.5),\n",
    "\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])\n",
    "\n",
    "# Also some transformations for validation and test datasets\n",
    "val_test_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(*stats)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e0675c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct classes: 15\n",
      "         plant                             disease\n",
      "0        Apple                          Apple_scab\n",
      "1        Apple                           Black_rot\n",
      "2        Apple                    Cedar_apple_rust\n",
      "3        Apple                             healthy\n",
      "4        Grape                           Black_rot\n",
      "5        Grape                Esca_(Black_Measles)\n",
      "6        Grape  Leaf_blight_(Isariopsis_Leaf_Spot)\n",
      "7        Grape                             healthy\n",
      "8        Peach                      Bacterial_spot\n",
      "9        Peach                             healthy\n",
      "10      Potato                        Early_blight\n",
      "11      Potato                         Late_blight\n",
      "12      Potato                             healthy\n",
      "13  Strawberry                         Leaf_scorch\n",
      "14  Strawberry                             healthy\n",
      "Mild samples: 5714 samples with 15 classes,\n",
      "Moderate: 2858 samples with 15 classes,\n",
      "Aggressive: 959 samples with 15 classes\n"
     ]
    }
   ],
   "source": [
    "all_data = pandas.concat([train_csv, val_csv, test_csv])\n",
    "master_classes = (\n",
    "    all_data[[\"plant\", \"disease\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values([\"plant\", \"disease\"], ignore_index=True)\n",
    ")\n",
    "\n",
    "# We divide the training dataset into 3 different augmentation levels:\n",
    "#   - 60% of them use mild augmentation\n",
    "#   - 30% of them use moderate augmentation\n",
    "#   - 10% of them use aggressive augmentation\n",
    "\n",
    "# Deterministically shuffle the training dataframe\n",
    "train_csv_shuffled = train_csv.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "# Lists to hold the dataframe chunks\n",
    "mild_chunks = []\n",
    "moderate_chunks = []\n",
    "aggressive_chunks = []\n",
    "\n",
    "# Group by (plant, disease) so we can split each class individually\n",
    "grouped = train_csv.groupby(['plant', 'disease'])\n",
    "\n",
    "for _, group in grouped:\n",
    "    num_samples = len(group)\n",
    "    mild_end = int(0.6 * num_samples)\n",
    "    moderate_end = int(0.9 * num_samples)\n",
    "\n",
    "    mild_chunks.append(group.iloc[:mild_end])\n",
    "    moderate_chunks.append(group.iloc[mild_end:moderate_end])\n",
    "    aggressive_chunks.append(group.iloc[moderate_end:])\n",
    "\n",
    "train_mild_df = pandas.concat(mild_chunks)\n",
    "train_moderate_df = pandas.concat(moderate_chunks)\n",
    "train_aggressive_df = pandas.concat(aggressive_chunks)\n",
    "\n",
    "train_ds_mild = ImgDataset(train_mild_df, TRAIN_IMG, transform=mild_transform, classes=master_classes)\n",
    "train_ds_moderate = ImgDataset(train_moderate_df, TRAIN_IMG, transform=moderate_transform, classes=master_classes)\n",
    "train_ds_aggressive = ImgDataset(train_aggressive_df, TRAIN_IMG, transform=aggressive_transform, classes=master_classes)\n",
    "\n",
    "train_ds = torch.utils.data.ConcatDataset([train_ds_mild, train_ds_moderate, train_ds_aggressive])\n",
    "val_ds = ImgDataset(val_csv, VAL_IMG, transform=val_test_transform, classes=master_classes)\n",
    "test_ds = ImgDataset(test_csv, TEST_IMG, transform=val_test_transform, classes=master_classes)\n",
    "\n",
    "num_classes = len(master_classes)\n",
    "\n",
    "print(f\"Total distinct classes: {len(master_classes)}\")\n",
    "print(train_ds_mild.classes)\n",
    "print(f\"Mild samples: {len(train_ds_mild)} samples with {len(train_ds_mild.classes)} classes,\\n\"\n",
    "      f\"Moderate: {len(train_ds_moderate)} samples with {len(train_ds_moderate.classes)} classes,\\n\"\n",
    "      f\"Aggressive: {len(train_ds_aggressive)} samples with {len(train_ds_aggressive.classes)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a56bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configs\n",
    "MODEL_DIR = os.path.join(\"/home/banana9205/Desktop/Main/Uni/DATH/models\")\n",
    "\n",
    "batch_size = 16\n",
    "lr = 2e-4\n",
    "weight_decay = 1e-3\n",
    "\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"ResNet18\",\n",
    "        \"model\": ResNet18(block=ResidualBlock, blocks_per_layer=[2, 2, 2, 2], n_channels=3, n_classes=num_classes),\n",
    "        \"epochs\": 10,\n",
    "        \"patience\": 3,\n",
    "        \"device\": device\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MobileNetV1\",\n",
    "        \"model\": MobileNetV1(n_channels=3, n_classes=num_classes),\n",
    "        \"epochs\": 15,\n",
    "        \"patience\": 5,\n",
    "        \"device\": device\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MobileNetV2\",\n",
    "        \"model\": MobileNetV2(n_channels=3, n_classes=num_classes),\n",
    "        \"epochs\": 15,\n",
    "        \"patience\": 5,\n",
    "        \"device\": device\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"VGG16\",\n",
    "        \"model\": VGG16(n_channels=3, n_classes=num_classes),\n",
    "        \"epochs\": 10,\n",
    "        \"patience\": 3,\n",
    "        \"device\": device\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9a0ae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    # Derive a unique seed for this worker based on the global seed\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Create a deterministic generator for the DataLoader\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)  # Uses the global 'seed' variable (42)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, \n",
    "                          shuffle=True, num_workers=4, \n",
    "                          worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "val_loader   = DataLoader(val_ds, batch_size=batch_size, \n",
    "                          shuffle=False, num_workers=4, \n",
    "                          worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, \n",
    "                          shuffle=False, num_workers=4, \n",
    "                          worker_init_fn=seed_worker, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb53816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config):\n",
    "    \"\"\"\n",
    "    A helper function to train and evaluate a model based on the provided configuration.\n",
    "    Args: config (dict): A dictionary containing the following keys:\n",
    "        - \"name\": Name of the model (str)\n",
    "        - \"model\": The PyTorch model instance (nn.Module)\n",
    "        - \"epochs\": Number of training epochs (int)\n",
    "        - \"patience\": Patience for early stopping (int)\n",
    "    \"\"\"\n",
    "    name = config[\"name\"]\n",
    "    model = config[\"model\"].to(device)\n",
    "    num_epochs = config[\"epochs\"]\n",
    "    patience = config[\"patience\"]\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\nTraining {name}\\n{'='*40}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_val_f1 = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"[Train] Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = correct / total * 100\n",
    "        \n",
    "        # Validation phase (Accuracy + F1 Score)\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = correct / total * 100\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        print(f\"Results: Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"         Train Acc:  {train_acc:.2f}%   | Val Acc:  {val_acc:.2f}%   | Val F1:   {val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            save_path = os.path.join(MODEL_DIR, f\"{name}_best.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve}/{patience} epochs.\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered for {name}!\")\n",
    "                break\n",
    "\n",
    "    return best_val_acc, best_val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21eb736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics of Models:\n"
     ]
    }
   ],
   "source": [
    "# Main training loop for all model configurations\n",
    "models_to_train = []\n",
    "results = {}\n",
    "\n",
    "for config in model_configs:\n",
    "    if config[\"name\"] not in models_to_train:\n",
    "        continue\n",
    "    val_acc, val_f1 = train_and_evaluate(config)\n",
    "    results[config[\"name\"]] = (val_acc, val_f1)\n",
    "    \n",
    "print(\"\\nFinal Metrics of Models:\")\n",
    "for name, (acc, f1) in results.items():\n",
    "    print(f\"{name}: {acc:.2f}% | {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "426f39dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Best Models on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing ResNet18: 100%|██████████| 127/127 [00:04<00:00, 30.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 Test Accuracy: 97.19%, Test F1 Score: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing MobileNetV1: 100%|██████████| 127/127 [00:03<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV1 Test Accuracy: 89.85%, Test F1 Score: 0.8988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing MobileNetV2: 100%|██████████| 127/127 [00:04<00:00, 31.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2 Test Accuracy: 94.38%, Test F1 Score: 0.9442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing phase\n",
    "models_to_test = [\"ResNet18\", \"MobileNetV1\", \"MobileNetV2\"]\n",
    "\n",
    "print(\"\\nTesting Best Models on Test Set:\")\n",
    "for config in model_configs:\n",
    "    name = config[\"name\"]\n",
    "    if name not in models_to_test:\n",
    "        continue\n",
    "    model = config[\"model\"].to(device)\n",
    "    load_path = os.path.join(MODEL_DIR, f\"{name}_best.pth\")\n",
    "    model.load_state_dict(torch.load(load_path))\n",
    "    model.eval()\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=f\"Testing {name}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_acc = correct / total * 100\n",
    "    test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print(f\"{name} Test Accuracy: {test_acc:.2f}%, Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c72779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model           | Params (M)   | Size (MB) \n",
      "ResNet18        | 11.18        | 42.70     \n",
      "MobileNetV1     | 3.22         | 12.38     \n",
      "MobileNetV2     | 2.24         | 8.69      \n"
     ]
    }
   ],
   "source": [
    "# Final model size and parameter count report\n",
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_all_mb\n",
    "\n",
    "print(f\"{'Model':<15} | {'Params (M)':<12} | {'Size (MB)':<10}\")\n",
    "\n",
    "for config in model_configs:\n",
    "    name = config[\"name\"]\n",
    "    if name not in models_to_test:\n",
    "        continue\n",
    "    model = config[\"model\"]\n",
    "    # Move to CPU for static calculation just in case\n",
    "    model.cpu()\n",
    "    \n",
    "    num_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    size_mb = get_model_size(model)\n",
    "    \n",
    "    print(f\"{config['name']:<15} | {num_params:<12.2f} | {size_mb:<10.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dath",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
